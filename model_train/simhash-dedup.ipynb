{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2dd0a50b-4469-4dfd-aa24-96ed7628c1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5b32bb21-d0a8-4941-85dd-9adb91625271",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14072"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('benzinga.csv')\n",
    "newsfeeds = df.to_json(orient='records')\n",
    "newsfeeds = json.loads(newsfeeds)\n",
    "len(newsfeeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa71d41c-512f-4c90-b9ad-874e63b28a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from simhash import Simhash, SimhashIndex\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc2d5188-3ad9-4ad0-a276-f4f2c21ce2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# omit the loeading messages, unlike the class exercise\n",
    "model_w2v_AP = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1532a697-ccb1-42f5-b3bc-ccf42896dd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function calculates similarity between two strings using a particular word vector model\n",
    "def calc_similarity(input1, input2, vectors):\n",
    "    s1words = set(vocab_check(vectors, input1.split()))\n",
    "    s2words = set(vocab_check(vectors, input2.split()))\n",
    "    \n",
    "    output = vectors.n_similarity(s1words, s2words)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3278f673-adfb-4f26-8244-2af03d70456c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a subset of 100 Webhose titles assign index values\n",
    "feeds = []\n",
    "i = 0\n",
    "for feed in newsfeeds[:]:\n",
    "    feed['id'] = i\n",
    "    #print(feed['id'], str(feed['title']))\n",
    "    i += 1\n",
    "    feeds.append(feed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "28cb4268-8946-4350-bd43-e86e8d5a7f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger('simhash').setLevel(logging.CRITICAL)\n",
    "\n",
    "hamming_distance = 10\n",
    "objs = [(str(feed['id']), Simhash(str(feed['title']))) for feed in feeds]\n",
    "index = SimhashIndex(objs, k=hamming_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3dff6a4d-20f0-4e94-9cc7-6cad81a5d626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this list keeps the first occurance of duplicated titles\n",
    "list_dupi = set()\n",
    "i = 0\n",
    "\n",
    "# loop to find all the duplicate\n",
    "while i < len(feeds):\n",
    "    feed_hash = Simhash(str(feeds[i]['title']))\n",
    "    dup_indices = index.get_near_dups(feed_hash)\n",
    "    if dup_indices:\n",
    "        dup_indices = sorted(set([int(ind) for ind in dup_indices]))\n",
    "        list_dupi.update( dup_indices[1:] )\n",
    "    i+=1\n",
    "    if i in list_dupi:\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "85c4fa07-9d41-490f-90df-9b6ff8b6f782",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1130"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_dupi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "60f6739c-72fc-4108-9610-6b549ccc5737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12942\n"
     ]
    }
   ],
   "source": [
    "feeds = [feed for feed in feeds if int(feed['id']) not in list_dupi]\n",
    "print(len(feeds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4cd86145-f135-4f85-bc6c-a37039481016",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"benzinga.json\"\n",
    "\n",
    "with open(file_path, \"w\") as file:\n",
    "    json.dump(feeds, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
